{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO :**\n",
    "- Recheck Augmentations\n",
    "- Recheck LAB normalization\n",
    "- sampler for faster convergence ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append(\"../code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from data.transforms import HE_preprocess\n",
    "from data.dataset import TileDataset\n",
    "\n",
    "from model_zoo.models import define_model\n",
    "\n",
    "from training.main import k_fold\n",
    "from utils.logger import (\n",
    "    prepare_log_folder,\n",
    "    save_config,\n",
    "    create_logger,\n",
    "    update_overall_logs,\n",
    ")\n",
    "\n",
    "from utils.plots import plot_contours\n",
    "\n",
    "from params import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.read_csv(DATA_PATH + f\"HuBMAP-20-dataset_information.csv\")\n",
    "df_mask = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "df = pd.read_csv(OUT_PATH + f\"df_images_{TILE_SIZE}_{REDUCE_FACTOR}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_weighting(size, sigma=1, alpha=1, eps=1e-6):\n",
    "    half = size // 2\n",
    "    w = np.ones((size, size), np.float32)\n",
    "\n",
    "    x = np.concatenate([np.mgrid[-half:0], np.mgrid[1: half + 1]])[:, None]\n",
    "    x = np.tile(x, (1, size))\n",
    "    x = half + 1 - np.abs(x)\n",
    "    y = x.T\n",
    "\n",
    "    w = np.minimum(x, y)\n",
    "    w = (w / w.max()) ** sigma\n",
    "    w = np.minimum(w, 1)\n",
    "\n",
    "    w = (w - np.min(w) + eps) / (np.max(w) - np.min(w) + eps)\n",
    "\n",
    "    w = np.where(w > alpha, 1, w)\n",
    "    w = w / alpha\n",
    "    w = np.clip(w, 1e-3, 1)\n",
    "\n",
    "    w = np.round(w, 3)\n",
    "    return w.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = get_tile_weighting(256)\n",
    "\n",
    "plt.matshow(w.astype(float))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.lovasz import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TileDataset(\n",
    "    df,\n",
    "    IMG_PATH,\n",
    "    MASK_PATH,\n",
    "    transforms=HE_preprocess(augment=True, visualize=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(\"Unet\", \"resnet18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, y = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symmetric_lovasz(pred, y.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZES = {\n",
    "    \"resnet18\": 64,\n",
    "    \"resnet34\": 32, \n",
    "    \"resnext50_32x4d\": 32, \n",
    "    \"se_resnext50_32x4d\": 32,\n",
    "    \"efficientnet-b1\": 16,\n",
    "    \"efficientnet-b2\": 16,\n",
    "    \"efficientnet-b3\": 16,\n",
    "    \"efficientnet-b4\": 16,\n",
    "    \"efficientnet-b5\": 16,\n",
    "    \"efficientnet-b6\": 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Parameters used for training\n",
    "    \"\"\"\n",
    "    \n",
    "    # General\n",
    "    seed = 42\n",
    "    verbose = 1\n",
    "    img_dir = IMG_PATH\n",
    "    mask_dir = MASK_PATH\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    save_weights = True\n",
    "    iter_per_epoch = 5000 #10000\n",
    "    \n",
    "    # Image size\n",
    "    train_tile_size = 256\n",
    "    reduce_factor = 4\n",
    "    on_spot_sampling = 0.9\n",
    "\n",
    "    # k-fold\n",
    "    cv_column = \"5fold\"\n",
    "    random_state = 0\n",
    "    selected_folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "    # Model\n",
    "    encoder = \"efficientnet-b5\"  # \"resnet18\" \"resnext50_32x4d\", \"resnet34\", \"efficientnet-b5\"\n",
    "    decoder = \"Unet\"  # \"Unet\", \"DeepLabV3Plus\"\n",
    "    encoder_weights = \"imagenet\"\n",
    "    num_classes = 1\n",
    "\n",
    "    # Training\n",
    "    loss = \"BCEWithLogitsLoss\"  # \"SoftDiceLoss\" / \"BCEWithLogitsLoss\"  / \"lovasz\"\n",
    "    activation = \"none\" if loss == \"lovasz\" else \"sigmoid\"\n",
    "\n",
    "    optimizer = \"Adam\"\n",
    "    batch_size = BATCH_SIZES[encoder]\n",
    "    \n",
    "         \n",
    "    if batch_size == 32:\n",
    "        epochs = 40\n",
    "    elif batch_size >= 6:\n",
    "        epochs = 30\n",
    "    elif batch_size == 8:\n",
    "        epochs = 20\n",
    "        \n",
    "    if train_tile_size == 512:\n",
    "        batch_size = int(batch_size/4)\n",
    "\n",
    "    lr = 1e-3\n",
    "    swa_first_epoch = 50\n",
    "\n",
    "    warmup_prop = 0.05\n",
    "    val_bs = batch_size * 2\n",
    "\n",
    "    first_epoch_eval = 0\n",
    "\n",
    "    # Inference\n",
    "    overlap_factor = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "log_folder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    log_folder = prepare_log_folder(LOG_PATH)\n",
    "    print(f\"Logging results to {log_folder}\")\n",
    "    config_df = save_config(Config, log_folder + \"config.json\")\n",
    "    df.to_csv(log_folder + \"data.csv\", index=False)\n",
    "    create_logger(directory=log_folder, name=\"logs.txt\")\n",
    "\n",
    "metrics = k_fold(Config, df, log_folder=log_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
