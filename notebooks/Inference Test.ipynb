{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "sys.path.append(\"../code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.torch import seed_everything\n",
    "from utils.torch import load_model_weights\n",
    "from model_zoo.models import define_model\n",
    "from data.transforms import HE_preprocess_test\n",
    "from utils.rle import rle_encode_less_memory, enc2mask\n",
    "from utils.plots import plot_thresh_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = DATA_PATH + 'test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_img_path,\n",
    "        df_info,\n",
    "        rle=None,\n",
    "        overlap_factor=1,\n",
    "        tile_size=256,\n",
    "        reduce_factor=4,\n",
    "        transforms=None,\n",
    "    ):\n",
    "        self.original_img = load_image(original_img_path, df_info, reduce_factor=reduce_factor)\n",
    "        self.orig_size = self.original_img.shape\n",
    "\n",
    "        self.raw_tile_size = tile_size\n",
    "        self.reduce_factor = reduce_factor\n",
    "        self.tile_size = tile_size\n",
    "\n",
    "        self.overlap_factor = overlap_factor\n",
    "\n",
    "        self.positions = self.get_positions()\n",
    "        self.transforms = transforms\n",
    "\n",
    "        if rle is not None:\n",
    "            self.mask = enc2mask(rle, (self.orig_size[1], self.orig_size[0])) > 0\n",
    "        else:\n",
    "            self.mask = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.positions)\n",
    "\n",
    "    def get_positions(self):\n",
    "        top_x = np.arange(\n",
    "            0,\n",
    "            self.orig_size[0],  # +self.tile_size,\n",
    "            int(self.tile_size / self.overlap_factor),\n",
    "        )\n",
    "        top_y = np.arange(\n",
    "            0,\n",
    "            self.orig_size[1],  # +self.tile_size,\n",
    "            int(self.tile_size / self.overlap_factor),\n",
    "        )\n",
    "        starting_positions = []\n",
    "        for x in top_x:\n",
    "            right_space = self.orig_size[0] - (x + self.tile_size)\n",
    "            if right_space > 0:\n",
    "                boundaries_x = (x, x + self.tile_size)\n",
    "            else:\n",
    "                boundaries_x = (x + right_space, x + right_space + self.tile_size)\n",
    "\n",
    "            for y in top_y:\n",
    "                down_space = self.orig_size[1] - (y + self.tile_size)\n",
    "                if down_space > 0:\n",
    "                    boundaries_y = (y, y + self.tile_size)\n",
    "                else:\n",
    "                    boundaries_y = (y + down_space, y + down_space + self.tile_size)\n",
    "                starting_positions.append((boundaries_x, boundaries_y))\n",
    "\n",
    "        return starting_positions\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pos_x, pos_y = self.positions[idx]\n",
    "        img = self.original_img[pos_x[0]: pos_x[1], pos_y[0]: pos_y[1], :]\n",
    "        \n",
    "        img = self.transforms(image=img)[\"image\"]\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile as tiff\n",
    "\n",
    "\n",
    "def load_image(img_path, df_info, reduce_factor=1):\n",
    "    \"\"\"\n",
    "    Load image and make sure sizes matches df_info\n",
    "    \"\"\"\n",
    "    image_fname = img_path.rsplit(\"/\", -1)[-1]\n",
    "    \n",
    "    W = int(df_info[df_info.image_file == image_fname][\"width_pixels\"])\n",
    "    H = int(df_info[df_info.image_file == image_fname][\"height_pixels\"])\n",
    "\n",
    "    img = tiff.imread(img_path).squeeze()\n",
    "\n",
    "    channel_pos = np.argwhere(np.array(img.shape) == 3)[0][0]\n",
    "    W_pos = np.argwhere(np.array(img.shape) == W)[0][0]\n",
    "    H_pos = np.argwhere(np.array(img.shape) == H)[0][0]\n",
    "\n",
    "    img = np.moveaxis(img, (H_pos, W_pos, channel_pos), (0, 1, 2))\n",
    "    \n",
    "    if reduce_factor > 1:\n",
    "        img = cv2.resize(\n",
    "            img,\n",
    "            (img.shape[1] // reduce_factor, img.shape[0] // reduce_factor),\n",
    "            interpolation=cv2.INTER_AREA,\n",
    "        )\n",
    "        \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def load_models(cp_folder):\n",
    "    config = json.load(open(cp_folder + 'config.json', 'r'))\n",
    "    config = Config(**config)\n",
    "    \n",
    "    weights = sorted(glob.glob(cp_folder + \"*.pt\"))\n",
    "    models = []\n",
    "    \n",
    "    for weight in weights:\n",
    "        model = define_model(\n",
    "            config.decoder,\n",
    "            config.encoder,\n",
    "            num_classes=config.num_classes,\n",
    "            encoder_weights=None,\n",
    "        )\n",
    "        \n",
    "        model = load_model_weights(model, weight)\n",
    "        models.append(model)\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def get_tile_weighting(size, sigma=1):\n",
    "    half = size // 2\n",
    "    w = np.ones((size, size), np.float32)\n",
    "\n",
    "    x = np.concatenate([np.mgrid[-half:0], np.mgrid[1: half + 1]])[:, None]\n",
    "    x = np.tile(x, (1, size))\n",
    "    x = half + 1 - np.abs(x)\n",
    "    y = x.T\n",
    "\n",
    "    w = np.minimum(x, y)\n",
    "    w = (w / w.max()) ** sigma\n",
    "    w = np.minimum(w, 1)\n",
    "\n",
    "    return w.astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def predict_entire_mask_no_thresholding(dataset, models, batch_size=8, upscale=True):\n",
    "    models = [model.to(DEVICE).eval() for model in models]\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    w = get_tile_weighting(dataset.tile_size, sigma=1)\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            model_preds = []\n",
    "            for model in models:\n",
    "                pred = model(batch.to(DEVICE))\n",
    "                _, _, h, w = pred.shape\n",
    "                \n",
    "                if upscale:\n",
    "                    pred = torch.nn.functional.interpolate(\n",
    "                        pred, (h * dataset.reduce_factor, w * dataset.reduce_factor)\n",
    "                    )\n",
    "\n",
    "                pred = pred.sigmoid().detach().cpu().view(-1, h, w).numpy()\n",
    "                model_preds.append(pred)\n",
    "                \n",
    "            model_preds = np.mean(model_preds, 0).astype(np.float16)\n",
    "            preds.append(model_preds)\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "\n",
    "    global_pred = np.zeros(\n",
    "        (dataset.orig_size[0], dataset.orig_size[1]), dtype=np.float16\n",
    "    )\n",
    "    global_counter = np.zeros(\n",
    "        (dataset.orig_size[0], dataset.orig_size[1]), dtype=np.float16\n",
    "    )\n",
    "\n",
    "    for tile_idx, (pos_x, pos_y) in enumerate(dataset.positions):\n",
    "        global_pred[pos_x[0]: pos_x[1], pos_y[0]: pos_y[1]] += preds[tile_idx, :, :] * w\n",
    "        global_counter[pos_x[0]: pos_x[1], pos_y[0]: pos_y[1]] += w\n",
    "\n",
    "    global_pred = np.divide(global_pred, global_counter).astype(np.float16)  # divide by overlapping tiles\n",
    "\n",
    "    return global_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions to RLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def threshold_resize_encode(preds, shape, threshold=0.5):\n",
    "    preds = (preds > threshold).astype(np.uint8)\n",
    "    \n",
    "    preds = cv2.resize(\n",
    "        preds,\n",
    "        (shape[0], shape[1]),\n",
    "        interpolation=cv2.INTER_AREA,\n",
    "    )\n",
    "    \n",
    "    preds = rle_encode_less_memory(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDUCE_FACTOR = 4\n",
    "THRESHOLD = 0.4\n",
    "\n",
    "CP_FOLDER = \"../dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH + 'sample_submission.csv')\n",
    "df_info = pd.read_csv(DATA_PATH + \"HuBMAP-20-dataset_information.csv\")\n",
    "rles = df_mask = pd.read_csv(DATA_PATH + \"train_4.csv\")\n",
    "\n",
    "config = json.load(open(CP_FOLDER + 'config.json', 'r'))\n",
    "config = Config(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading weights from ../dataset/Unet_efficientnet-b5_0.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../dataset/Unet_efficientnet-b5_1.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../dataset/Unet_efficientnet-b5_2.pt\n",
      "\n",
      "\n",
      " -> Loading weights from ../dataset/Unet_efficientnet-b5_3.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = load_models(CP_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t Image 2f6ecfcdf\n",
      "\n",
      " - Building dataset\n",
      "\n",
      " - Predicting masks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd438ae18e74eb7b3ed878caf952021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=396.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Scored 0.9493 with threshold 0.40\n",
      "\n",
      " - Resizing & encoding\n"
     ]
    }
   ],
   "source": [
    "for img in df['id'].unique():\n",
    "    \n",
    "    if DEBUG:\n",
    "        # Check performances on a validation image\n",
    "        img = \"2f6ecfcdf\"\n",
    "        IMG_PATH = DATA_PATH + \"train\"\n",
    "        models = models[:1]\n",
    "    \n",
    "    print(f'\\n\\t Image {img}')\n",
    "    \n",
    "    print(f'\\n - Building dataset')\n",
    "    \n",
    "    rle = rles[rles['id'] == img][\"encoding\"] if DEBUG else None\n",
    "    \n",
    "    predict_dataset = InferenceDataset(\n",
    "        f\"{IMG_PATH}/{img}.tiff\",\n",
    "        df_info,\n",
    "        rle=rle,\n",
    "        overlap_factor=config.overlap_factor,\n",
    "        reduce_factor=REDUCE_FACTOR,\n",
    "        transforms=HE_preprocess_test(augment=False, visualize=False),\n",
    "    )\n",
    "    \n",
    "    print(f'\\n - Predicting masks')\n",
    "\n",
    "    global_pred = predict_entire_mask_no_thresholding(\n",
    "        predict_dataset, models, batch_size=config.val_bs // 4, upscale=False\n",
    "    )\n",
    "    \n",
    "    if DEBUG:\n",
    "        threshold, score = plot_thresh_scores(\n",
    "            mask=predict_dataset.mask, pred=global_pred, plot=False\n",
    "        ) \n",
    "        print(f\" -> Scored {score :.4f} with threshold {threshold:.2f}\")\n",
    "    \n",
    "    print('\\n - Resizing & encoding')\n",
    "    \n",
    "    shape = df_info[df_info.image_file == img + \".tiff\"][['width_pixels', 'height_pixels']].values.astype(int)[0]\n",
    "    rle = threshold_resize_encode(global_pred, shape, threshold=THRESHOLD)\n",
    "    df.loc[df.id == img, 'predicted'] = rle\n",
    "    \n",
    "    del global_pred, predict_dataset\n",
    "    gc.collect()\n",
    "    \n",
    "    if DEBUG:\n",
    "        break\n",
    "    \n",
    "# df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2ec3f1bb9</td>\n",
       "      <td>60762295 20 60786285 20 60810275 20 60834265 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3589adb90</td>\n",
       "      <td>68658950 68 68688383 68 68717816 68 68747249 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d488c759a</td>\n",
       "      <td>548575513 60 548622173 60 548668833 60 5487154...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa05346ff</td>\n",
       "      <td>52856681 48 52887401 48 52918121 48 52948841 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57512b7f1</td>\n",
       "      <td>328952557 28 328985797 28 329019037 28 3290522...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                          predicted\n",
       "0  2ec3f1bb9  60762295 20 60786285 20 60810275 20 60834265 2...\n",
       "1  3589adb90  68658950 68 68688383 68 68717816 68 68747249 6...\n",
       "2  d488c759a  548575513 60 548622173 60 548668833 60 5487154...\n",
       "3  aa05346ff  52856681 48 52887401 48 52918121 48 52948841 4...\n",
       "4  57512b7f1  328952557 28 328985797 28 329019037 28 3290522..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
