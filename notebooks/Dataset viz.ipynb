{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO :**\n",
    "- Bigger img input / bigger mask\n",
    "- mixup\n",
    "- use oof to remove noisy annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sys.path.append(\"../code/\")\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"1,0\"\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.main import k_fold\n",
    "\n",
    "from utils.logger import (\n",
    "    prepare_log_folder,\n",
    "    save_config,\n",
    "    create_logger,\n",
    "    update_overall_logs,\n",
    ")\n",
    "\n",
    "from params import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.read_csv(DATA_PATH + f\"HuBMAP-20-dataset_information.csv\")\n",
    "df_mask = pd.read_csv(DATA_PATH + \"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZES = {\n",
    "    \"resnet18\": 64,\n",
    "    \"resnet34\": 32,\n",
    "    \"resnext50_32x4d\": 32,\n",
    "    \"se_resnext50_32x4d\": 32,\n",
    "    \"efficientnet-b0\": 32,\n",
    "    \"efficientnet-b1\": 32,\n",
    "    \"efficientnet-b2\": 32,\n",
    "    \"efficientnet-b3\": 16,\n",
    "    \"efficientnet-b4\": 16,\n",
    "    \"efficientnet-b5\": 16,\n",
    "    \"efficientnet-b6\": 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Parameters used for training\n",
    "    \"\"\"\n",
    "    # General\n",
    "    seed = 42\n",
    "    verbose = 1\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    save_weights = True\n",
    "    sampling_mode = 'centered' # chose between 'convhull', 'centered', 'random', 'visible'\n",
    "\n",
    "    # Images\n",
    "    tile_size = 512\n",
    "    reduce_factor = 2\n",
    "    on_spot_sampling = 0.99\n",
    "    overlap_factor = 1.5\n",
    "\n",
    "    img_dir = DATA_PATH + f\"train_{tile_size}_red_{reduce_factor}\"\n",
    "    mask_dir = DATA_PATH + f\"masks_{tile_size}_red_{reduce_factor}\"\n",
    "\n",
    "    # k-fold\n",
    "    cv_column = \"5fold\"\n",
    "    random_state = 0\n",
    "    selected_folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "    # Model\n",
    "    encoder = \"efficientnet-b1\"  # \"resnet18\" \"resnext50_32x4d\", \"resnet34\", \"efficientnet-b5\"\n",
    "    decoder = \"Unet\"  # \"Unet\", \"DeepLabV3Plus\"\n",
    "    use_bot = False\n",
    "    use_fpn = False\n",
    "    double_model = False\n",
    "    use_mixstyle = False\n",
    "    encoder_weights = \"imagenet\"\n",
    "    num_classes = 2\n",
    "\n",
    "    # Training\n",
    "    loss = \"BCEWithLogitsLoss\"  # \"SoftDiceLoss\" / \"BCEWithLogitsLoss\"  / \"lovasz\"\n",
    "    activation = \"none\" if loss == \"lovasz\" else \"sigmoid\"\n",
    "\n",
    "    optimizer = \"Adam\"\n",
    "\n",
    "    batch_size = BATCH_SIZES[encoder]\n",
    "\n",
    "    if tile_size == 512:\n",
    "        batch_size = batch_size // 2\n",
    "\n",
    "    if batch_size >= 32:\n",
    "        epochs = 50 \n",
    "    elif batch_size >= 16:\n",
    "        epochs = 40\n",
    "    elif batch_size >= 6:\n",
    "        epochs = 30\n",
    "    else:\n",
    "        epochs = 25\n",
    "\n",
    "    iter_per_epoch = 5000\n",
    "    lr = 1e-3\n",
    "    swa_first_epoch = 50\n",
    "\n",
    "    warmup_prop = 0.05\n",
    "    val_bs = batch_size * 2\n",
    "\n",
    "    first_epoch_eval = 0\n",
    "\n",
    "    mix_proba = 0\n",
    "    mix_alpha = 0.4\n",
    "\n",
    "    if mix_proba > 0:\n",
    "        epochs *= 3\n",
    "\n",
    "    use_fp16 = True\n",
    "\n",
    "    oof_folder = None  # \"../logs/2021-04-14/0/\"\n",
    "    loss_oof_weight = 0  # 0.5\n",
    "\n",
    "    use_pl = 0.25\n",
    "    use_external = 0.25 \n",
    "\n",
    "    predict_fc = False\n",
    "    if predict_fc:\n",
    "        pl_path = \"../output/submission_0938.csv\"  # replace with one that predicts even more fcs ?\n",
    "        extra_path = \"../input/train_extra_fc.csv\"\n",
    "        rle_paths = f\"../input/train_{reduce_factor}_fc.csv\"\n",
    "    else:\n",
    "        pl_path = \"../logs/2021-05-01/2/\"\n",
    "        extra_path = [\n",
    "            \"../input/train_extra.csv\",\n",
    "            \"../input/train_extra_onlyfc.csv\"\n",
    "        ]\n",
    "        rle_path = [\n",
    "            f\"../input/train_{reduce_factor}_fix.csv\",\n",
    "            f\"../input/train_{reduce_factor}_onlyfc.csv\",\n",
    "        ]\n",
    "\n",
    "    if use_pl > 0:\n",
    "        epochs += 10\n",
    "        \n",
    "#     epochs = 1\n",
    "#     iter_per_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import InMemoryTrainDataset\n",
    "from data.transforms import HE_preprocess\n",
    "\n",
    "def get_dataset(config, log_folder=None):\n",
    "    \"\"\"\n",
    "    Performs a patient grouped k-fold cross validation.\n",
    "    The following things are saved to the log folder : val predictions, histories\n",
    "\n",
    "    Args:\n",
    "        config (Config): Parameters.\n",
    "        log_folder (None or str, optional): Folder to logs results to. Defaults to None.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    nb_folds = 5\n",
    "    # Data preparation\n",
    "    print(\"Creating in-memory dataset ...\")\n",
    "\n",
    "    if isinstance(config.rle_path, list):\n",
    "        df_rle = [pd.read_csv(path) for path in config.rle_path]\n",
    "        train_img_names = df_rle[0].id.unique()\n",
    "    else:\n",
    "        df_rle = pd.read_csv(config.rle_path)\n",
    "        train_img_names = df_rle.id.unique()\n",
    "\n",
    "    if isinstance(config.extra_path, list):\n",
    "        df_rle_extra = [pd.read_csv(path) for path in config.extra_path]\n",
    "    else:\n",
    "        df_rle_extra = pd.read_csv(config.extra_path)\n",
    "\n",
    "    in_mem_dataset = InMemoryTrainDataset(\n",
    "        train_img_names,\n",
    "        df_rle,\n",
    "        train_tile_size=config.tile_size,\n",
    "        reduce_factor=config.reduce_factor,\n",
    "        train_transfo=HE_preprocess(augment=False,size=config.tile_size, visualize=True),\n",
    "        valid_transfo=HE_preprocess(augment=False, size=config.tile_size, visualize=True),\n",
    "        train_path=f\"../input/train_{config.reduce_factor}/\",\n",
    "        iter_per_epoch=config.iter_per_epoch,\n",
    "        on_spot_sampling=config.on_spot_sampling,\n",
    "        sampling_mode=config.sampling_mode,\n",
    "        oof_folder=config.oof_folder,\n",
    "        pl_path=config.pl_path,\n",
    "        use_pl=config.use_pl,\n",
    "        test_path=f\"../input/test_{config.reduce_factor}/\",\n",
    "        df_rle_extra=df_rle_extra,\n",
    "        use_external=config.use_external,\n",
    "    )\n",
    "    \n",
    "    return in_mem_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = get_dataset(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.update_fold_nb(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.train_img_names[8] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in np.random.choice(len(dataset), 1000):\n",
    "    img, mask, _, w = dataset[i]\n",
    "    \n",
    "    if mask[:, :, 1].sum() and w:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(img.numpy().transpose(1, 2, 0))\n",
    "        plt.axis(False)\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(mask[:, :, 0].numpy())\n",
    "        plt.colorbar()\n",
    "        plt.axis(False)\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(mask[:, :, 1].numpy())\n",
    "        plt.colorbar()\n",
    "        plt.axis(False)\n",
    "        plt.show()\n",
    "        \n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
